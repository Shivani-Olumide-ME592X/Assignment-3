{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_3jRU4p6Znz",
        "outputId": "362b8b17-09d6-4a39-8d23-be0da306bbee"
      },
      "source": [
        "from tensorflow import keras\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from google.colab import drive \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import re\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dropout,Dense,BatchNormalization,Flatten\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l65KnW0t7AZz"
      },
      "source": [
        "shutil.unpack_archive(\"/content/gdrive/My Drive/Leaf_Images.zip\", \"/content/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM2_oZo7-ZAN"
      },
      "source": [
        "folders=os.listdir('/content/Leaf_Images/')\n",
        "for i in folders:\n",
        "  files=os.listdir('/content/Leaf_Images/'+i+'/output/')\n",
        "  for j in files:\n",
        "    os.rename('/content/Leaf_Images/' +i+ '/output/' +j, '/content/Leaf_Images/' +i+ '/' +j)\n",
        "  os.rmdir('/content/Leaf_Images/' +i+ '/output/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gum2azUM0kHM"
      },
      "source": [
        "for i in sorted(folders):\n",
        "  if(i!='.ipynb_checkpoints'):\n",
        "    if(int(i)>1 ):\n",
        "      os.rename('/content/Leaf_Images/'+str(i),'/content/Leaf_Images/'+str(int(i)-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLcCtLVtC3cq"
      },
      "source": [
        "folders=os.listdir('/content/Leaf_Images/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ9-vouRa5_4"
      },
      "source": [
        "if(not os.path.exists('/content/Leaf_Images/train')):\n",
        "  os.makedirs('/content/Leaf_Images/train')\n",
        "if(not os.path.exists('/content/Leaf_Images/valid')):\n",
        "  os.makedirs('/content/Leaf_Images/valid')\n",
        "if(not os.path.exists('/content/Leaf_Images/test')):\n",
        "  os.makedirs('/content/Leaf_Images/test')\n",
        "for i in folders:\n",
        "  if(i!='.ipynb_checkpoints'):\n",
        "    if(not os.path.exists('/content/Leaf_Images/train/'+i)):\n",
        "        os.makedirs('/content/Leaf_Images/train/'+i)\n",
        "    if(not os.path.exists('/content/Leaf_Images/valid/'+i)):\n",
        "        os.makedirs('/content/Leaf_Images/valid/'+i)\n",
        "    if(not os.path.exists('/content/Leaf_Images/test/'+i)):\n",
        "        os.makedirs('/content/Leaf_Images/test/'+i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLXbs6X5ZvT8"
      },
      "source": [
        "for i in folders:\n",
        "  files=os.listdir('/content/Leaf_Images/'+i)\n",
        "  length_1=int(0.7*len(files))\n",
        "  length_2=int(0.85*len(files)) \n",
        "  for j in files[:length_1]:\n",
        "    if(j!='.ipynb_checkpoints' and i!='.ipynb_checkpoints'):\n",
        "      os.rename('/content/Leaf_Images/' +i+ '/' +j,'/content/Leaf_Images/train/'+i+'/'+j)\n",
        "  for j in files[length_1:length_2]:\n",
        "    if(j!='.ipynb_checkpoints' and i!='.ipynb_checkpoints'):\n",
        "      os.rename('/content/Leaf_Images/' +i+ '/' +j,'/content/Leaf_Images/valid/'+i+'/'+j)\n",
        "  for j in files[length_2:]:\n",
        "    if(j!='.ipynb_checkpoints' and i!='.ipynb_checkpoints'):\n",
        "      os.rename('/content/Leaf_Images/' +i+ '/' +j,'/content/Leaf_Images/test/'+i+'/'+j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsO-_z4X0hM6"
      },
      "source": [
        "folders=os.listdir('/content/Leaf_Images/')\n",
        "fdrs=['train','valid','test']\n",
        "for i in folders:\n",
        "  if(i not in fdrs):\n",
        "    os.rmdir('/content/Leaf_Images/'+i)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLJRJqcOc2WI",
        "outputId": "cd292df2-9053-4cb6-9610-a1c9a5741853"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        fill_mode=\"nearest\",\n",
        "        brightness_range=[0.2,1.0],\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/Leaf_Images/train',\n",
        "        target_size=(120, 120),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "valid_generator = test_datagen.flow_from_directory(\n",
        "        '/content/Leaf_Images/valid',\n",
        "        target_size=(120, 120),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10303 images belonging to 7 classes.\n",
            "Found 2209 images belonging to 7 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiAas6z1c9hG",
        "outputId": "81e36942-461d-4257-a5b8-b76f1cb64c8e"
      },
      "source": [
        "model=Sequential()\n",
        "input_shape=(120,120,3)\n",
        "model.add(Conv2D(32,(3,3),padding='same',activation='relu',input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64,(3,3),padding='same',activation='relu',input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64,(3,3),padding='same',activation='relu',input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(128,(3,3),padding='same',activation='relu',input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128,(3,3),padding='same',activation='relu',input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7,activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 120, 120, 32)      896       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 120, 120, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 40, 40, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 40, 40, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 40, 40, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 40, 40, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 40, 40, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 40, 40, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 13, 13, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 13, 13, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 13, 13, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 13, 13, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 7)                 7175      \n",
            "=================================================================\n",
            "Total params: 2,388,871\n",
            "Trainable params: 2,385,991\n",
            "Non-trainable params: 2,880\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLOMqQBUt6iD",
        "outputId": "78ba6b89-0fb0-48d6-b261-bae8a11c216f"
      },
      "source": [
        "optimizer= tf.optimizers.Adam(lr=1e-3)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
        "history=model.fit(train_generator,epochs=30, shuffle=True, validation_data=valid_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "322/322 [==============================] - 474s 1s/step - loss: 2.0364 - accuracy: 0.3842 - val_loss: 4.7047 - val_accuracy: 0.3010\n",
            "Epoch 2/30\n",
            "322/322 [==============================] - 442s 1s/step - loss: 1.2796 - accuracy: 0.5706 - val_loss: 2.1585 - val_accuracy: 0.4572\n",
            "Epoch 3/30\n",
            "322/322 [==============================] - 438s 1s/step - loss: 1.0121 - accuracy: 0.6556 - val_loss: 1.0030 - val_accuracy: 0.7035\n",
            "Epoch 4/30\n",
            "322/322 [==============================] - 441s 1s/step - loss: 0.8393 - accuracy: 0.7005 - val_loss: 0.8437 - val_accuracy: 0.6931\n",
            "Epoch 5/30\n",
            "322/322 [==============================] - 440s 1s/step - loss: 0.7893 - accuracy: 0.7255 - val_loss: 1.1336 - val_accuracy: 0.7189\n",
            "Epoch 6/30\n",
            "322/322 [==============================] - 440s 1s/step - loss: 0.7448 - accuracy: 0.7459 - val_loss: 0.5547 - val_accuracy: 0.8112\n",
            "Epoch 7/30\n",
            "322/322 [==============================] - 440s 1s/step - loss: 0.6884 - accuracy: 0.7574 - val_loss: 0.8867 - val_accuracy: 0.7184\n",
            "Epoch 8/30\n",
            "322/322 [==============================] - 439s 1s/step - loss: 0.6336 - accuracy: 0.7826 - val_loss: 0.7701 - val_accuracy: 0.7623\n",
            "Epoch 9/30\n",
            "322/322 [==============================] - 442s 1s/step - loss: 0.6098 - accuracy: 0.7870 - val_loss: 0.7819 - val_accuracy: 0.7750\n",
            "Epoch 10/30\n",
            "322/322 [==============================] - 442s 1s/step - loss: 0.5482 - accuracy: 0.8060 - val_loss: 0.9825 - val_accuracy: 0.7402\n",
            "Epoch 11/30\n",
            "322/322 [==============================] - 438s 1s/step - loss: 0.4994 - accuracy: 0.8220 - val_loss: 0.8881 - val_accuracy: 0.7519\n",
            "Epoch 12/30\n",
            "322/322 [==============================] - 437s 1s/step - loss: 0.5523 - accuracy: 0.8036 - val_loss: 0.7553 - val_accuracy: 0.7759\n",
            "Epoch 13/30\n",
            "322/322 [==============================] - 436s 1s/step - loss: 0.4813 - accuracy: 0.8286 - val_loss: 0.6727 - val_accuracy: 0.7918\n",
            "Epoch 14/30\n",
            "322/322 [==============================] - 438s 1s/step - loss: 0.4456 - accuracy: 0.8451 - val_loss: 0.7351 - val_accuracy: 0.7741\n",
            "Epoch 15/30\n",
            "322/322 [==============================] - 438s 1s/step - loss: 0.4705 - accuracy: 0.8356 - val_loss: 1.3771 - val_accuracy: 0.6759\n",
            "Epoch 16/30\n",
            "322/322 [==============================] - 438s 1s/step - loss: 0.4721 - accuracy: 0.8387 - val_loss: 2.1152 - val_accuracy: 0.5143\n",
            "Epoch 17/30\n",
            "322/322 [==============================] - 439s 1s/step - loss: 0.4734 - accuracy: 0.8270 - val_loss: 0.4575 - val_accuracy: 0.8592\n",
            "Epoch 18/30\n",
            "322/322 [==============================] - 439s 1s/step - loss: 0.4442 - accuracy: 0.8459 - val_loss: 0.4207 - val_accuracy: 0.8483\n",
            "Epoch 19/30\n",
            "322/322 [==============================] - 438s 1s/step - loss: 0.4321 - accuracy: 0.8465 - val_loss: 1.2341 - val_accuracy: 0.6700\n",
            "Epoch 20/30\n",
            "322/322 [==============================] - 437s 1s/step - loss: 0.4078 - accuracy: 0.8550 - val_loss: 0.6485 - val_accuracy: 0.8117\n",
            "Epoch 21/30\n",
            "322/322 [==============================] - 438s 1s/step - loss: 0.3965 - accuracy: 0.8570 - val_loss: 0.6677 - val_accuracy: 0.8053\n",
            "Epoch 22/30\n",
            "322/322 [==============================] - 442s 1s/step - loss: 0.3576 - accuracy: 0.8709 - val_loss: 0.6869 - val_accuracy: 0.7904\n",
            "Epoch 23/30\n",
            "322/322 [==============================] - 446s 1s/step - loss: 0.3909 - accuracy: 0.8558 - val_loss: 1.6795 - val_accuracy: 0.6053\n",
            "Epoch 24/30\n",
            "322/322 [==============================] - 442s 1s/step - loss: 0.4334 - accuracy: 0.8440 - val_loss: 0.4005 - val_accuracy: 0.8723\n",
            "Epoch 25/30\n",
            "322/322 [==============================] - 441s 1s/step - loss: 0.3903 - accuracy: 0.8594 - val_loss: 0.8041 - val_accuracy: 0.7619\n",
            "Epoch 26/30\n",
            "322/322 [==============================] - 441s 1s/step - loss: 0.3771 - accuracy: 0.8601 - val_loss: 0.6022 - val_accuracy: 0.7990\n",
            "Epoch 27/30\n",
            "322/322 [==============================] - 441s 1s/step - loss: 0.3586 - accuracy: 0.8722 - val_loss: 0.4308 - val_accuracy: 0.8502\n",
            "Epoch 28/30\n",
            "322/322 [==============================] - 439s 1s/step - loss: 0.3477 - accuracy: 0.8749 - val_loss: 0.5358 - val_accuracy: 0.8366\n",
            "Epoch 29/30\n",
            "322/322 [==============================] - 439s 1s/step - loss: 0.3576 - accuracy: 0.8708 - val_loss: 0.6259 - val_accuracy: 0.8094\n",
            "Epoch 30/30\n",
            "322/322 [==============================] - 436s 1s/step - loss: 0.3407 - accuracy: 0.8766 - val_loss: 0.5841 - val_accuracy: 0.8108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1tUa9SRCLya"
      },
      "source": [
        "model.save(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JxZx94OIbKv"
      },
      "source": [
        "Hyperparamter Tuning:\n",
        "\n",
        "The CNN classifiers were built for bounding boxes with different sizes 120 x 120, 100 x 100 and 80 x 80\n",
        "\n",
        "The learning rate was initially used was around the range 1e-3 and when the learning rate was switched to 3e-4, a learning rate which usually works best with Adam optimizer, there was a dip in accuracy by 10%.\n",
        "\n",
        "This could be accounted to the fact that the slow learning rate takes longer to converge but is more probable to end up in a global minima. Owing to time and resource constraints, each model was trained only for 20-30 epochs. With increased epochs, a learning rate of 3e-4 would provide better accuracy.\n",
        "\n",
        "Also, changing the depth of the network by a maximum of 2 layers showed no impact (showed just 1-2 % improvement in accuracy)\n",
        "\n",
        "Changing the drop out values showed similar results."
      ]
    }
  ]
}